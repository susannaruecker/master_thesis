{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-german-cased' # 'distilbert-base-german-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Das', 'hier', 'ist', 'ein', 'deutscher', 'Beispiel', '##text', '.', 'Und', 'ein', 'zweiter', '.']\n",
      "[295, 702, 127, 39, 2433, 2249, 8859, 26914, 1356, 39, 8266, 26914]\n",
      "[3, 295, 702, 127, 39, 2433, 2249, 8859, 26914, 1356, 39, 8266, 26914, 4]\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Das hier ist ein deutscher Beispieltext. Und ein zweiter.\"\n",
    "tokens = tokenizer.tokenize(sample_text) # just tokenizes\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids = tokenizer.encode(sample_text) # already adds special tokens\n",
    "\n",
    "print(tokens)\n",
    "print(token_ids)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[SEP]', 4, '[CLS]', 3, '[PAD]', 0)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token, tokenizer.sep_token_id, tokenizer.cls_token, tokenizer.cls_token_id, tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path('/Volumes/INWT/Daten_NLP/') # encrypted folder!\n",
    "DATA = ROOT / '200707_aachener_zeitung_modified.csv' # text is already minimal preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class INWT_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, target, text_base, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.text_base = text_base\n",
    "        self.target = target\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.df.loc[item, self.text_base])\n",
    "        target = np.array(self.df.loc[item, self.target])\n",
    "\n",
    "        # hier einfach encode() nehmen? brauche ich die attention_mask etc?\n",
    "        encoding = self.tokenizer.encode_plus(text,\n",
    "                                              max_length=self.max_len,\n",
    "                                              truncation=True,\n",
    "                                              #return_token_type_ids=False,\n",
    "                                              pad_to_max_length=True,\n",
    "                                              return_attention_mask=True,\n",
    "                                              return_tensors='pt',\n",
    "                                              )\n",
    "\n",
    "        return {'text': text,\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'target': torch.tensor(target, dtype=torch.float).unsqueeze(dim=-1) # unsqueezing so shape (batch_size,1)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleId</th>\n",
       "      <th>pageviews</th>\n",
       "      <th>entrances</th>\n",
       "      <th>exits</th>\n",
       "      <th>bounces</th>\n",
       "      <th>timeOnPage</th>\n",
       "      <th>conversions</th>\n",
       "      <th>avgTimeOnPage</th>\n",
       "      <th>stickiness</th>\n",
       "      <th>entranceRate</th>\n",
       "      <th>...</th>\n",
       "      <th>titelH3</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>category</th>\n",
       "      <th>city</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>nr_tokens</th>\n",
       "      <th>mean_token_length</th>\n",
       "      <th>nr_tokens_teaser</th>\n",
       "      <th>nr_tokens_titelH1</th>\n",
       "      <th>avgTimeOnPage/wordcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48620281</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1012</td>\n",
       "      <td></td>\n",
       "      <td>112.444444</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>769</td>\n",
       "      <td>vm</td>\n",
       "      <td>München/Stuttgart</td>\n",
       "      <td>Frische Luft und Bewegung: Diese Kombination r...</td>\n",
       "      <td>796</td>\n",
       "      <td>5.359296</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>0.146222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48620381</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1484</td>\n",
       "      <td></td>\n",
       "      <td>185.500000</td>\n",
       "      <td>42.105263</td>\n",
       "      <td>31.578947</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>441</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin/Frankfurt/Main</td>\n",
       "      <td>Der Wecker klingelt, aufstehen! Doch gerade im...</td>\n",
       "      <td>452</td>\n",
       "      <td>5.938053</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>0.420635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48622639</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>390</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Eltern auf der Suche nach einem guten Babyphon...</td>\n",
       "      <td>396</td>\n",
       "      <td>5.848485</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48623085</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>974</td>\n",
       "      <td></td>\n",
       "      <td>81.166667</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>28.125000</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>345</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Spülmaschinentabs sollen kleine Alleskönner se...</td>\n",
       "      <td>367</td>\n",
       "      <td>5.594005</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>0.235266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48623259</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3797</td>\n",
       "      <td></td>\n",
       "      <td>223.352941</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>182</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Make-up hat heutzutage einen Zweck: Es soll da...</td>\n",
       "      <td>183</td>\n",
       "      <td>5.622951</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>1.227214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   articleId  pageviews  entrances  exits  bounces  timeOnPage conversions  \\\n",
       "0   48620281         21          7     12        7        1012               \n",
       "1   48620381         19          6     11        5        1484               \n",
       "2   48622639          2          2      2        2           0               \n",
       "3   48623085         32          9     20        9         974               \n",
       "4   48623259         24          2      7        2        3797               \n",
       "\n",
       "   avgTimeOnPage  stickiness  entranceRate  ...  titelH3 wordcount category  \\\n",
       "0     112.444444   42.857143     33.333333  ...                769       vm   \n",
       "1     185.500000   42.105263     31.578947  ...                441       vm   \n",
       "2       0.000000    0.000000    100.000000  ...                390       vm   \n",
       "3      81.166667   37.500000     28.125000  ...                345       vm   \n",
       "4     223.352941   70.833333      8.333333  ...                182       vm   \n",
       "\n",
       "                    city                                  text_preprocessed  \\\n",
       "0      München/Stuttgart  Frische Luft und Bewegung: Diese Kombination r...   \n",
       "1  Berlin/Frankfurt/Main  Der Wecker klingelt, aufstehen! Doch gerade im...   \n",
       "2                 Berlin  Eltern auf der Suche nach einem guten Babyphon...   \n",
       "3                 Berlin  Spülmaschinentabs sollen kleine Alleskönner se...   \n",
       "4                 Berlin  Make-up hat heutzutage einen Zweck: Es soll da...   \n",
       "\n",
       "  nr_tokens mean_token_length nr_tokens_teaser nr_tokens_titelH1  \\\n",
       "0       796          5.359296               29                 9   \n",
       "1       452          5.938053               33                 8   \n",
       "2       396          5.848485               30                 7   \n",
       "3       367          5.594005               30                 7   \n",
       "4       183          5.622951               22                 8   \n",
       "\n",
       "  avgTimeOnPage/wordcount  \n",
       "0                0.146222  \n",
       "1                0.420635  \n",
       "2                0.000000  \n",
       "3                0.235266  \n",
       "4                1.227214  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(DATA)\n",
    "df_raw = df_raw.fillna('') # replacing Nan with emtpy string\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667\n"
     ]
    }
   ],
   "source": [
    "# just take articles where ...\n",
    "#df = df_raw.loc[(df_raw['pageviews'] >= 5)]\n",
    "\n",
    "df = df_raw.loc[(df_raw['pageviews'] >= 5) &\n",
    "                (df_raw['avgTimeOnPage/wordcount'] <= 5) &\n",
    "                (df_raw['avgTimeOnPage/wordcount'] >= 0.1)]\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(533, 36) (67, 36) (67, 36)\n"
     ]
    }
   ],
   "source": [
    "#creating train, dev, test\n",
    "RANDOM_SEED = 123\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_dev, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_train.reset_index(drop=True, inplace=True) # so that index starts with 0 again\n",
    "df_dev.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "print(df_train.shape, df_dev.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleId</th>\n",
       "      <th>pageviews</th>\n",
       "      <th>entrances</th>\n",
       "      <th>exits</th>\n",
       "      <th>bounces</th>\n",
       "      <th>timeOnPage</th>\n",
       "      <th>conversions</th>\n",
       "      <th>avgTimeOnPage</th>\n",
       "      <th>stickiness</th>\n",
       "      <th>entranceRate</th>\n",
       "      <th>...</th>\n",
       "      <th>titelH3</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>category</th>\n",
       "      <th>city</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>nr_tokens</th>\n",
       "      <th>mean_token_length</th>\n",
       "      <th>nr_tokens_teaser</th>\n",
       "      <th>nr_tokens_titelH1</th>\n",
       "      <th>avgTimeOnPage/wordcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48859325</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2002</td>\n",
       "      <td></td>\n",
       "      <td>286.000000</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>578</td>\n",
       "      <td>vm</td>\n",
       "      <td>Hamburg</td>\n",
       "      <td>Weiße Wände, helle Möbel und wenige Accessoire...</td>\n",
       "      <td>594</td>\n",
       "      <td>5.558923</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>0.494810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50185755</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2947</td>\n",
       "      <td></td>\n",
       "      <td>163.722222</td>\n",
       "      <td>62.068966</td>\n",
       "      <td>3.448276</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>533</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Maske auf beim Einkaufen oder in Bus und Bahn:...</td>\n",
       "      <td>523</td>\n",
       "      <td>5.502868</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>0.307171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49278955</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td></td>\n",
       "      <td>74.000000</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>252</td>\n",
       "      <td>vm</td>\n",
       "      <td>Stuttgart</td>\n",
       "      <td>Oldtimer sind gut in Schuss. Je älter die Auto...</td>\n",
       "      <td>253</td>\n",
       "      <td>5.407115</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>0.293651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49875299</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>622</td>\n",
       "      <td></td>\n",
       "      <td>77.750000</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>402</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Der Drill-Instuctor brüllt in sein Mikrofon, d...</td>\n",
       "      <td>413</td>\n",
       "      <td>5.372881</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>0.193408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48885853</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>576</td>\n",
       "      <td></td>\n",
       "      <td>82.285714</td>\n",
       "      <td>41.176471</td>\n",
       "      <td>41.176471</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>520</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Die «Aida Nova»: 2500 Kabinen. Die «Mein Schif...</td>\n",
       "      <td>535</td>\n",
       "      <td>5.685981</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>0.158242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   articleId  pageviews  entrances  exits  bounces  timeOnPage conversions  \\\n",
       "0   48859325         15          8      8        7        2002               \n",
       "1   50185755         29          1     11        1        2947               \n",
       "2   49278955          7          1      3        1         296               \n",
       "3   49875299         12          1      4        1         622               \n",
       "4   48885853         17          7     10        6         576               \n",
       "\n",
       "   avgTimeOnPage  stickiness  entranceRate  ...  titelH3 wordcount category  \\\n",
       "0     286.000000   46.666667     53.333333  ...                578       vm   \n",
       "1     163.722222   62.068966      3.448276  ...                533       vm   \n",
       "2      74.000000   57.142857     14.285714  ...                252       vm   \n",
       "3      77.750000   66.666667      8.333333  ...                402       vm   \n",
       "4      82.285714   41.176471     41.176471  ...                520       vm   \n",
       "\n",
       "        city                                  text_preprocessed nr_tokens  \\\n",
       "0    Hamburg  Weiße Wände, helle Möbel und wenige Accessoire...       594   \n",
       "1     Berlin  Maske auf beim Einkaufen oder in Bus und Bahn:...       523   \n",
       "2  Stuttgart  Oldtimer sind gut in Schuss. Je älter die Auto...       253   \n",
       "3     Berlin  Der Drill-Instuctor brüllt in sein Mikrofon, d...       413   \n",
       "4     Berlin  Die «Aida Nova»: 2500 Kabinen. Die «Mein Schif...       535   \n",
       "\n",
       "  mean_token_length nr_tokens_teaser nr_tokens_titelH1 avgTimeOnPage/wordcount  \n",
       "0          5.558923               31                 5                0.494810  \n",
       "1          5.502868               27                11                0.307171  \n",
       "2          5.407115               25                 7                0.293651  \n",
       "3          5.372881               27                10                0.193408  \n",
       "4          5.685981               29                 8                0.158242  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DataLoaders(target, text_base, tokenizer, max_len, batch_size):\n",
    "    # creating DataSets\n",
    "    ds_train = INWT_Dataset(df=df_train,\n",
    "                  target = target,\n",
    "                  text_base = text_base,\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_len = max_len)\n",
    "    ds_dev = INWT_Dataset(df=df_dev,\n",
    "                  target = target,\n",
    "                  text_base = text_base,\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_len = max_len)\n",
    "    ds_test = INWT_Dataset(df=df_test,\n",
    "                  target = target,\n",
    "                  text_base = text_base,\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_len = max_len)\n",
    "    \n",
    "    # creating DataLoaders\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size)#, num_workers=4)\n",
    "    dl_dev = DataLoader(ds_dev, batch_size=batch_size)#, num_workers=4)\n",
    "    dl_test = DataLoader(ds_test, batch_size=batch_size)#, num_workers=4)\n",
    "    \n",
    "    return dl_train, dl_dev, dl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_dev, dl_test = create_DataLoaders(target = 'avgTimeOnPage/wordcount',\n",
    "                                               text_base = 'text_preprocessed', # 'titelH1',\n",
    "                                               tokenizer = tokenizer, \n",
    "                                               max_len = 50,            # change depending on used text_base!\n",
    "                                               batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'input_ids', 'attention_mask', 'target'])\n",
      "tensor([[    3, 19206, 18285,  ..., 26914,     2,     4],\n",
      "        [    3,  8359,   772,  ..., 26901,   287,     4],\n",
      "        [    3, 19024, 21258,  ...,  4140, 10687,     4],\n",
      "        ...,\n",
      "        [    3, 10796,   961,  ..., 20665,  5032,     4],\n",
      "        [    3,  1309,   498,  ...,   204,  2197,     4],\n",
      "        [    3,   198,  8630,  ..., 12122, 15333,     4]])\n",
      "torch.Size([32, 50])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "torch.Size([32, 50])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# have a look at one batch\n",
    "data = next(iter(dl_train))\n",
    "print(data.keys())\n",
    "input_ids = data['input_ids']\n",
    "print(input_ids)\n",
    "print(input_ids.shape)\n",
    "attention_mask = data['attention_mask']\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)\n",
    "print(data['target'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# das ist umständlich (und auch falsch), ich habe stattdessen bereits BertForSequenceClassification genommen\n",
    "# https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "\n",
    "#class Bert_regression(nn.Module):\n",
    "#    \n",
    "#    def __init__(self, n_outputs): # maybe train pageviews and timeOnPage simultaneously?\n",
    "#        super(Bert_regression, self).__init__()\n",
    "#        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "#        self.drop = nn.Dropout(p=0.3)\n",
    "#        self.out = nn.Linear(self.bert.config.hidden_size, n_outputs)\n",
    "#\n",
    "#    def forward(self, input_ids, attention_mask):\n",
    "#        _, pooled_output = self.bert(input_ids=input_ids,           # das hier ist glaube ich nicht sinnvoll bei mir\n",
    "#                                     attention_mask=attention_mask)\n",
    "#        output = self.drop(pooled_output)\n",
    "#   \n",
    "#        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Bert_regression(n_outputs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME,\n",
    "                                                      num_labels = 1, # turns \"classification\" into regression?\n",
    "                                                      output_attentions = False,\n",
    "                                                      output_hidden_states = False,\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0722],\n",
       "         [-0.2745],\n",
       "         [-0.2264],\n",
       "         [-0.3081],\n",
       "         [-0.0234],\n",
       "         [-0.3807],\n",
       "         [-0.2915],\n",
       "         [-0.3310],\n",
       "         [-0.5103],\n",
       "         [-0.0482],\n",
       "         [-0.3297],\n",
       "         [-0.3032],\n",
       "         [-0.2080],\n",
       "         [-0.2565],\n",
       "         [-0.4692],\n",
       "         [-0.0347],\n",
       "         [-0.2431],\n",
       "         [-0.0141],\n",
       "         [-0.2056],\n",
       "         [-0.4307],\n",
       "         [-0.1809],\n",
       "         [-0.2822],\n",
       "         [-0.4090],\n",
       "         [-0.2577],\n",
       "         [-0.2373],\n",
       "         [-0.2155],\n",
       "         [-0.1640],\n",
       "         [-0.3332],\n",
       "         [-0.3144],\n",
       "         [-0.2914],\n",
       "         [-0.1223],\n",
       "         [-0.2668]], grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try ut with one batch (model is not trained yet so nothing exciting to be expected)\n",
    "model(input_ids, attention_mask) # semms good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.MSELoss()  # mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "training\n",
      "-Batch 6\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-262-46110620a069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#print(targets.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# stimmt das so? ist [0] die logits?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#print(outputs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/work/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/work/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m         )\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/work/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/work/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         )\n\u001b[1;32m    764\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/work/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/work/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                 )\n\u001b[1;32m    441\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/work/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/work/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    369\u001b[0m     ):\n\u001b[1;32m    370\u001b[0m         self_attention_outputs = self.attention(\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         )\n\u001b[1;32m    373\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/work/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/work/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    313\u001b[0m     ):\n\u001b[1;32m    314\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         )\n\u001b[1;32m    317\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/work/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/work/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    ### TRAINING on train\n",
    "    print(\"training\")\n",
    "    model = model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for nr, d in enumerate(dl_train):\n",
    "        print(\"-Batch\", nr, end='\\r')\n",
    "        input_ids = d[\"input_ids\"]\n",
    "        attention_mask = d[\"attention_mask\"]\n",
    "        targets = d[\"target\"]\n",
    "        #print(targets.shape)    \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)[0] # stimmt das so? ist [0] die logits?\n",
    "        #print(outputs.shape)\n",
    "        \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        #print(np.mean(train_losses))\n",
    "    print(\"Mean train loss:\", np.mean(train_losses))\n",
    "    \n",
    "    ### EVALUATING on dev\n",
    "    print(\"evaluating\")\n",
    "    model = model.eval()\n",
    "    eval_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for nr, d in enumerate(dl_dev):\n",
    "            print(\"-Batch\", nr, end='\\r')\n",
    "            input_ids = d[\"input_ids\"]\n",
    "            attention_mask = d[\"attention_mask\"]\n",
    "            targets = d[\"target\"]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)[0] # stimmt das so?\n",
    "        \n",
    "            loss = loss_fn(outputs, targets)\n",
    "            eval_losses.append(loss.item())\n",
    "            #print(np.mean(eval_losses))\n",
    "        print(\"Mean eval loss:\", np.mean(eval_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dl_dev_oneBatch, _ = create_DataLoaders(target = 'avgTimeOnPage/wordcount',\n",
    "                                               text_base = 'text_preprocessed', # 'teaser'\n",
    "                                               tokenizer = tokenizer, \n",
    "                                               max_len = 100,            # change depending on used text_base!\n",
    "                                               batch_size = 67) # for now: just one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.2933, 12.3086, 12.3034, 12.2823, 12.2906, 12.2941, 12.3136, 12.2987,\n",
      "        12.2861, 12.2873, 12.2507, 12.2997, 12.2819, 12.2853, 12.3088, 12.2795,\n",
      "        12.2993, 12.2814, 12.2881, 12.3007, 12.2934, 12.2879, 12.2927, 12.2814,\n",
      "        12.2832, 12.3043, 12.3022, 12.2989, 12.2966, 12.3052, 12.2808, 12.3051,\n",
      "        12.2821, 12.2927, 12.2764, 12.2815, 12.2991, 12.2824, 12.2783, 12.2737,\n",
      "        12.2980, 12.3045, 12.2785, 12.2990, 12.2982, 12.3028, 12.2725, 12.2922,\n",
      "        12.2856, 12.2970, 12.2983, 12.2904, 12.2839, 12.2974, 12.2993, 12.2653,\n",
      "        12.3095, 12.2745, 12.2972, 12.2916, 12.3087, 12.2794, 12.2994, 12.2933,\n",
      "        12.2878, 12.3078, 12.2545, 12.2969, 12.2767, 12.2955, 12.3029, 12.3008,\n",
      "        12.2982, 12.2705, 12.2950, 12.2845, 12.2903, 12.3097, 12.3059, 12.2845,\n",
      "        12.2968, 12.2984, 12.3076, 12.3052, 12.2739, 12.2729, 12.2879, 12.3032,\n",
      "        12.2816])\n",
      "tensor([3.0470e+03, 3.3000e+01, 2.0000e+01, 6.4000e+01, 1.0000e+00, 1.2000e+01,\n",
      "        2.0000e+00, 2.2000e+01, 3.0000e+00, 6.0000e+00, 1.0000e+00, 1.9000e+01,\n",
      "        3.0000e+00, 2.6200e+02, 9.0000e+00, 4.0000e+00, 2.4000e+01, 1.0000e+02,\n",
      "        2.7000e+01, 1.2400e+02, 3.0000e+00, 8.0000e+00, 1.4000e+01, 6.0000e+00,\n",
      "        3.9000e+01, 1.4000e+01, 1.6000e+01, 1.8000e+01, 9.0000e+00, 8.0000e+00,\n",
      "        8.0000e+00, 9.0000e+00, 1.6000e+01, 3.2000e+01, 1.9000e+01, 8.0000e+00,\n",
      "        2.0000e+01, 3.0000e+00, 1.8000e+01, 1.1000e+01, 1.8000e+01, 9.0000e+00,\n",
      "        9.0000e+00, 2.4000e+01, 1.4000e+01, 6.0000e+00, 5.0000e+00, 1.1500e+02,\n",
      "        4.0000e+00, 1.4000e+01, 9.0000e+00, 7.0000e+00, 1.6000e+01, 1.7000e+01,\n",
      "        6.0000e+00, 8.0000e+00, 1.9000e+01, 3.9000e+01, 1.7000e+01, 2.9000e+01,\n",
      "        2.4000e+01, 1.3000e+01, 1.0000e+00, 8.0000e+00, 1.0000e+01, 3.0000e+00,\n",
      "        7.0000e+00, 1.4000e+01, 9.0000e+00, 7.0000e+00, 7.0000e+00, 2.0000e+00,\n",
      "        7.0000e+00, 4.0000e+00, 1.2000e+01, 5.0000e+00, 1.2000e+01, 7.0000e+00,\n",
      "        1.7300e+02, 1.5000e+01, 3.0000e+00, 1.8000e+01, 2.9000e+01, 2.1000e+01,\n",
      "        1.0000e+00, 1.0000e+01, 2.0000e+00, 9.0000e+00, 1.8000e+01])\n",
      "(0.022001277673926255, 0.8378454215220879)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for d in dl_dev_oneBatch:\n",
    "        pred_dev = model(input_ids=d[\"input_ids\"], attention_mask=d[\"attention_mask\"])[0] # just the logits?\n",
    "        y_dev = d[\"target\"]\n",
    "        \n",
    "        pred_dev = pred_dev.squeeze()\n",
    "        y_dev = y_dev.squeeze()\n",
    "        print(pred_dev)\n",
    "        print(y_dev)\n",
    "        print(st.pearsonr(pred_dev, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kommentare:\n",
    "* Das Modell lernt nicht! Liegt das einfach an der geringen Datenmenge oder ist im Training was falsch?\n",
    "* anderer Optimizer? Ist Loss richtig berechnet?\n",
    "* Oder liegt das Problem bereits beim DataSet, Dataloader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
