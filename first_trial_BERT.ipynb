{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-german-cased' # 'distilbert-base-german-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Das', 'hier', 'ist', 'ein', 'deutscher', 'Beispiel', '##text', '.', 'Und', 'ein', 'zweiter', '.']\n",
      "[295, 702, 127, 39, 2433, 2249, 8859, 26914, 1356, 39, 8266, 26914]\n",
      "[3, 295, 702, 127, 39, 2433, 2249, 8859, 26914, 1356, 39, 8266, 26914, 4]\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Das hier ist ein deutscher Beispieltext. Und ein zweiter.\"\n",
    "tokens = tokenizer.tokenize(sample_text) # just tokenizes\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids = tokenizer.encode(sample_text) # already adds special tokens\n",
    "\n",
    "print(tokens)\n",
    "print(token_ids)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[SEP]', 4, '[CLS]', 3, '[PAD]', 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token, tokenizer.sep_token_id, tokenizer.cls_token, tokenizer.cls_token_id, tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path('/Volumes/INWT/Daten_NLP/') # encrypted folder!\n",
    "DATA = ROOT / '200707_aachener_zeitung_modified.csv' # text is already minimal preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class INWT_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, target, text_base, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.text_base = text_base\n",
    "        self.target = target\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.df.loc[item, self.text_base])\n",
    "        target = np.array(self.df.loc[item, self.target])\n",
    "\n",
    "        # hier einfach encode() nehmen? brauche ich die attention_mask etc?\n",
    "        encoding = self.tokenizer.encode_plus(text,\n",
    "                                              max_length=self.max_len,\n",
    "                                              truncation=True,\n",
    "                                              #return_token_type_ids=False,\n",
    "                                              pad_to_max_length=True,\n",
    "                                              return_attention_mask=True,\n",
    "                                              return_tensors='pt',\n",
    "                                              )\n",
    "\n",
    "        return {'text': text,\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'target': torch.tensor(target, dtype=torch.float).unsqueeze(dim=-1) # unsqueezing so shape (batch_size,1)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleId</th>\n",
       "      <th>pageviews</th>\n",
       "      <th>entrances</th>\n",
       "      <th>exits</th>\n",
       "      <th>bounces</th>\n",
       "      <th>timeOnPage</th>\n",
       "      <th>conversions</th>\n",
       "      <th>avgTimeOnPage</th>\n",
       "      <th>stickiness</th>\n",
       "      <th>entranceRate</th>\n",
       "      <th>...</th>\n",
       "      <th>titelH3</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>category</th>\n",
       "      <th>city</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>avgTimeOnPage/wordcount</th>\n",
       "      <th>nr_tokens</th>\n",
       "      <th>mean_token_length</th>\n",
       "      <th>nr_tokens_teaser</th>\n",
       "      <th>nr_tokens_titelH1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48620281</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1012</td>\n",
       "      <td></td>\n",
       "      <td>112.444444</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>769</td>\n",
       "      <td>vm</td>\n",
       "      <td>München/Stuttgart</td>\n",
       "      <td>Frische Luft und Bewegung: Diese Kombination r...</td>\n",
       "      <td>0.146222</td>\n",
       "      <td>796</td>\n",
       "      <td>5.359296</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48620381</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1484</td>\n",
       "      <td></td>\n",
       "      <td>185.500000</td>\n",
       "      <td>42.105263</td>\n",
       "      <td>31.578947</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>441</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin/Frankfurt/Main</td>\n",
       "      <td>Der Wecker klingelt, aufstehen! Doch gerade im...</td>\n",
       "      <td>0.420635</td>\n",
       "      <td>452</td>\n",
       "      <td>5.938053</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48622639</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>390</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Eltern auf der Suche nach einem guten Babyphon...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>396</td>\n",
       "      <td>5.848485</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48623085</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>974</td>\n",
       "      <td></td>\n",
       "      <td>81.166667</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>28.125000</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>345</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Spülmaschinentabs sollen kleine Alleskönner se...</td>\n",
       "      <td>0.235266</td>\n",
       "      <td>367</td>\n",
       "      <td>5.594005</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48623259</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3797</td>\n",
       "      <td></td>\n",
       "      <td>223.352941</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>182</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Make-up hat heutzutage einen Zweck: Es soll da...</td>\n",
       "      <td>1.227214</td>\n",
       "      <td>183</td>\n",
       "      <td>5.622951</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   articleId  pageviews  entrances  exits  bounces  timeOnPage conversions  \\\n",
       "0   48620281         21          7     12        7        1012               \n",
       "1   48620381         19          6     11        5        1484               \n",
       "2   48622639          2          2      2        2           0               \n",
       "3   48623085         32          9     20        9         974               \n",
       "4   48623259         24          2      7        2        3797               \n",
       "\n",
       "   avgTimeOnPage  stickiness  entranceRate  ...  titelH3 wordcount category  \\\n",
       "0     112.444444   42.857143     33.333333  ...                769       vm   \n",
       "1     185.500000   42.105263     31.578947  ...                441       vm   \n",
       "2       0.000000    0.000000    100.000000  ...                390       vm   \n",
       "3      81.166667   37.500000     28.125000  ...                345       vm   \n",
       "4     223.352941   70.833333      8.333333  ...                182       vm   \n",
       "\n",
       "                    city                                  text_preprocessed  \\\n",
       "0      München/Stuttgart  Frische Luft und Bewegung: Diese Kombination r...   \n",
       "1  Berlin/Frankfurt/Main  Der Wecker klingelt, aufstehen! Doch gerade im...   \n",
       "2                 Berlin  Eltern auf der Suche nach einem guten Babyphon...   \n",
       "3                 Berlin  Spülmaschinentabs sollen kleine Alleskönner se...   \n",
       "4                 Berlin  Make-up hat heutzutage einen Zweck: Es soll da...   \n",
       "\n",
       "  avgTimeOnPage/wordcount nr_tokens mean_token_length nr_tokens_teaser  \\\n",
       "0                0.146222       796          5.359296               29   \n",
       "1                0.420635       452          5.938053               33   \n",
       "2                0.000000       396          5.848485               30   \n",
       "3                0.235266       367          5.594005               30   \n",
       "4                1.227214       183          5.622951               22   \n",
       "\n",
       "  nr_tokens_titelH1  \n",
       "0                 9  \n",
       "1                 8  \n",
       "2                 7  \n",
       "3                 7  \n",
       "4                 8  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA)\n",
    "df = df.fillna('') # replacing Nan with emtpy string\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 36) (89, 36) (90, 36)\n"
     ]
    }
   ],
   "source": [
    "#creating train, dev, test\n",
    "RANDOM_SEED = 123\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_dev, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_train.reset_index(drop=True, inplace=True) # so that index starts with 0 again\n",
    "df_dev.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "print(df_train.shape, df_dev.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleId</th>\n",
       "      <th>pageviews</th>\n",
       "      <th>entrances</th>\n",
       "      <th>exits</th>\n",
       "      <th>bounces</th>\n",
       "      <th>timeOnPage</th>\n",
       "      <th>conversions</th>\n",
       "      <th>avgTimeOnPage</th>\n",
       "      <th>stickiness</th>\n",
       "      <th>entranceRate</th>\n",
       "      <th>...</th>\n",
       "      <th>titelH3</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>category</th>\n",
       "      <th>city</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>avgTimeOnPage/wordcount</th>\n",
       "      <th>nr_tokens</th>\n",
       "      <th>mean_token_length</th>\n",
       "      <th>nr_tokens_teaser</th>\n",
       "      <th>nr_tokens_titelH1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49583837</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>3680</td>\n",
       "      <td></td>\n",
       "      <td>262.857143</td>\n",
       "      <td>50.0</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>630</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Viele Deutsche dürften bis zuletzt die Hoffnun...</td>\n",
       "      <td>0.417234</td>\n",
       "      <td>655</td>\n",
       "      <td>6.097710</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51204297</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>417</td>\n",
       "      <td></td>\n",
       "      <td>139.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>80</td>\n",
       "      <td>vm</td>\n",
       "      <td>Stuttgart</td>\n",
       "      <td>Geht in einem Auto ein Assistenzsystem kaputt,...</td>\n",
       "      <td>1.737500</td>\n",
       "      <td>77</td>\n",
       "      <td>6.363636</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49230331</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>171</td>\n",
       "      <td></td>\n",
       "      <td>57.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>127</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Vor der Unterschrift unter den Bauvertrag brau...</td>\n",
       "      <td>0.448819</td>\n",
       "      <td>124</td>\n",
       "      <td>6.201613</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49825661</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1501</td>\n",
       "      <td></td>\n",
       "      <td>500.333333</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>176</td>\n",
       "      <td>vm</td>\n",
       "      <td>Losheim am See</td>\n",
       "      <td>Beim Kauf von Kettenspray können Motorradbesit...</td>\n",
       "      <td>2.842803</td>\n",
       "      <td>178</td>\n",
       "      <td>5.528090</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48897105</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td></td>\n",
       "      <td>41.333333</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>149</td>\n",
       "      <td>vm</td>\n",
       "      <td>Krefeld</td>\n",
       "      <td>Wer häufiger Schmerzen im Gesicht hat, kann Ze...</td>\n",
       "      <td>0.277405</td>\n",
       "      <td>146</td>\n",
       "      <td>6.410959</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   articleId  pageviews  entrances  exits  bounces  timeOnPage conversions  \\\n",
       "0   49583837         28          8     14        7        3680               \n",
       "1   51204297          4          0      1        0         417               \n",
       "2   49230331          4          1      1        1         171               \n",
       "3   49825661          5          2      2        2        1501               \n",
       "4   48897105          5          2      2        2         124               \n",
       "\n",
       "   avgTimeOnPage  stickiness  entranceRate  ...  titelH3 wordcount category  \\\n",
       "0     262.857143        50.0     28.571429  ...                630       vm   \n",
       "1     139.000000        75.0      0.000000  ...                 80       vm   \n",
       "2      57.000000        75.0     25.000000  ...                127       vm   \n",
       "3     500.333333        60.0     40.000000  ...                176       vm   \n",
       "4      41.333333        60.0     40.000000  ...                149       vm   \n",
       "\n",
       "             city                                  text_preprocessed  \\\n",
       "0          Berlin  Viele Deutsche dürften bis zuletzt die Hoffnun...   \n",
       "1       Stuttgart  Geht in einem Auto ein Assistenzsystem kaputt,...   \n",
       "2          Berlin  Vor der Unterschrift unter den Bauvertrag brau...   \n",
       "3  Losheim am See  Beim Kauf von Kettenspray können Motorradbesit...   \n",
       "4         Krefeld  Wer häufiger Schmerzen im Gesicht hat, kann Ze...   \n",
       "\n",
       "  avgTimeOnPage/wordcount nr_tokens mean_token_length nr_tokens_teaser  \\\n",
       "0                0.417234       655          6.097710               27   \n",
       "1                1.737500        77          6.363636               24   \n",
       "2                0.448819       124          6.201613               24   \n",
       "3                2.842803       178          5.528090               28   \n",
       "4                0.277405       146          6.410959               25   \n",
       "\n",
       "  nr_tokens_titelH1  \n",
       "0                 7  \n",
       "1                 6  \n",
       "2                 6  \n",
       "3                 8  \n",
       "4                 7  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DataLoaders(target, text_base, tokenizer, max_len, batch_size):\n",
    "    # creating DataSets\n",
    "    ds_train = INWT_Dataset(df=df_train,\n",
    "                  target = target,\n",
    "                  text_base = text_base,\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_len = max_len)\n",
    "    ds_dev = INWT_Dataset(df=df_dev,\n",
    "                  target = target,\n",
    "                  text_base = text_base,\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_len = max_len)\n",
    "    ds_test = INWT_Dataset(df=df_test,\n",
    "                  target = target,\n",
    "                  text_base = text_base,\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_len = max_len)\n",
    "    \n",
    "    # creating DataLoaders\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size)#, num_workers=4)\n",
    "    dl_dev = DataLoader(ds_dev, batch_size=batch_size)#, num_workers=4)\n",
    "    dl_test = DataLoader(ds_test, batch_size=batch_size)#, num_workers=4)\n",
    "    \n",
    "    return dl_train, dl_dev, dl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_dev, dl_test = create_DataLoaders(target = 'pageviews',\n",
    "                                               text_base = 'teaser', # 'titelH1',\n",
    "                                               tokenizer = tokenizer, \n",
    "                                               max_len = 30,            # change depending on used text_base!\n",
    "                                               batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'input_ids', 'attention_mask', 'target'])\n",
      "tensor([[    3,   125, 11032, 23293,  1561,  4766, 26918,  1575,    30,  3836,\n",
      "         21582, 15767,    21,  6471,  4045, 10184,  2026,  6352,    50,  9371,\n",
      "          1565, 26897,  4877,     7, 26914,  1309,  3984, 14069,   142,     4],\n",
      "        [    3, 14092,  7857,  1641,   144,   114,   225,  8389,    88,  3503,\n",
      "         26088,   147,  8959,     7, 26914,   311,  5503,   287,   620,  2734,\n",
      "           149, 26914, 13510,   213,   104,  8090, 26918,  1475,   213,     4],\n",
      "        [    3,  1309,    39,  2843, 10630,  1279, 26918,   259,  1146,    39,\n",
      "          1020,    81,  9695, 26918,  1547, 24174,   167, 26914,   311, 26500,\n",
      "          4347,   718,    30,   788, 14069,  5976, 26914,  2019,   788,     4],\n",
      "        [    3,   125, 20072,   443, 13809, 26902,  1080,  7746,    42,  7746,\n",
      "         26935,    42,  1616,  6633, 16266,  1281,    80, 26914,  5372,   127,\n",
      "         16449,  5032,  2945, 26914,   152,  5569, 18192,  1760, 21414,     4],\n",
      "        [    3,  3506, 22330,   287,   155,  2238, 11853, 26917,   484, 26935,\n",
      "         23645, 22330,  3077,   149, 26914,   864, 11946,   287,   213,   386,\n",
      "          2177,   155,  1856, 10703, 26914, 18840,   127,   670, 26918,     4],\n",
      "        [    3,  1309,    39,  1020, 10630,  1279, 26918,  1547,  2013, 14723,\n",
      "         26901, 24931, 26914,   198,  3313,    50,    86,  8325,    42,   115,\n",
      "            30, 16177, 17399, 24680, 26898, 19233,     7,   200, 26914,     4],\n",
      "        [    3,   173,    21,  3518,   474, 20883,  4585,  4826,   106,  3236,\n",
      "          5052,  9152, 19365,  1578, 26914, 18840,   116,  1685,   142, 11946,\n",
      "         26964, 16995, 26901,    30, 11959,  1868,    39, 26972,     4,     0],\n",
      "        [    3, 24318,   309,  2932,   903,  2923,   260,  2213,  1425,   246,\n",
      "            93,  1703,    42,    21, 12460,  3249, 26914,  3303,  8170, 26897,\n",
      "           718, 23413,  2551,   255, 19011,  1727,   167, 26935,   961,     4],\n",
      "        [    3,  1161, 22330,  7885,   181,  1522,   246,  4434,   946, 26914,\n",
      "          1679,   557,    30, 18846,  5460,    27,   252,   959,    54,   292,\n",
      "            42, 16748,   104,  2782,  4812,  9077, 26918, 13473,  5280,     4],\n",
      "        [    3,   371,  2265,   147,   246, 13955,  9068, 26918,   287,   386,\n",
      "           147, 18292,  1376, 26964, 16388,  2654, 19708, 26914,   125,  3032,\n",
      "           815, 26940,   508, 23371,   903, 25143,   309,    50,  6258,     4],\n",
      "        [    3,   198,  5644,   785,  7698,  5560, 26972, 26406, 26897,  1575,\n",
      "         26935,    50,   297, 12381,   203, 26914,  2573,    93,   149,  9476,\n",
      "         26918,   893,   229,    86,  5644,  3981,  2350,    21, 11161,     4],\n",
      "        [    3,  2251,  2617,  3356,  5513,   800,  6352,   260, 17623, 18749,\n",
      "           306,  6340, 26914, 11427, 26900,  1903,   716,  6812,   142,   303,\n",
      "          6071, 11219, 26912, 17075, 26914,  1025, 26913,   478,    93,     4],\n",
      "        [    3, 15615,  2343,   142,   380, 19676, 26920, 26918,  1835,   380,\n",
      "           142,  2343,  4857, 26964,  2147,  2116,  1867,   178,  2053, 15938,\n",
      "           893,   229,  1480, 20152, 26914,  1679,  6928,  2938,   682,     4],\n",
      "        [    3,   198,  9564,   127,  3278,   170,  2212, 26951, 26918,   386,\n",
      "            81, 18369,  1547,  9641,   282,   149, 23371,   903, 26914, 13672,\n",
      "          1575, 26918,   968,   229,   235,  7155, 26918,   309,    67,     4],\n",
      "        [    3,     2,  6285, 26901,   144,   883,    85,   104, 26918,   386,\n",
      "           127,   764,   149, 14593,     2, 26935,   142, 19816,   479,   620,\n",
      "          9207, 13221,   167, 26914,  4334,  6471,  4045,  1547,    93,     4],\n",
      "        [    3, 13362,  2794,   863,    81,    86,   781, 23415,    91,  1667,\n",
      "           115, 10712, 26918,  1302,    39,  2559, 17092, 26900,  5798,   343,\n",
      "           110,   811, 11578,   104, 26914,  1857,  2950,   213,   194,     4],\n",
      "        [    3,   295, 16341,   802,   185, 16906,    42,  4939,   574,    80,\n",
      "         26918,    30,  3744,  6483,    81, 26914,  5072, 15717,    39,   130,\n",
      "          2366,   280, 21872, 26972,  8137,  4522, 26964, 14510,   149,     4],\n",
      "        [    3,  1309,   498, 25898,  9954,  1279, 26918, 14584,  2734,   114,\n",
      "          4450,  1687,  2715, 26914,  1882,   246,  2648,   229,   114,  9005,\n",
      "           147, 26972, 15209,  3330,    93, 13765,   785,  1020, 12191,     4],\n",
      "        [    3,   233,  4880, 22514,    30, 23800,  2288,   564, 26914,   831,\n",
      "          3356,   718,    88, 21968, 12207, 26898, 15575, 26935,  3373,   557,\n",
      "           213,    88,    21,  3770,    39,    55, 26935,  3512,   252,     4],\n",
      "        [    3,   233,  6323,   127,  4938, 26918,    42,    30,  2579,   718,\n",
      "           303,   149,   246, 20780,  8839, 26972,   125,  6471,  4045, 26935,\n",
      "          5402,   492,    12, 20931,  2709, 18267, 10162, 13518, 26914,     4],\n",
      "        [    3,  3459,    30, 18486,  8518,  2093,   303, 14861, 26972, 12290,\n",
      "         26918,    93,   479,   194, 10131,  2860, 26914,  1309,   188,    21,\n",
      "         23970,   142,   303,  1280,  6323,   357, 23090,  1435,   447,     4],\n",
      "        [    3,   917, 19338,   293, 10742,  7564,    27, 26918,   557,    21,\n",
      "         11546, 23877,   372, 26935,  3061,    93,  3843,  4786, 26972,  6802,\n",
      "         26918,   386,   235,  3875,   356,   188,  2559, 17092, 26900,     4],\n",
      "        [    3,  1316,  2265,  3303,    87,   755,    47, 18574, 26935,    42,\n",
      "         10631,   486, 13516,  2033,   147, 26972,  4352,    93,  3505,  5715,\n",
      "         21523, 26918,   474,   213,  4574,  2941,    42,  5795, 15065,     4],\n",
      "        [    3,   864,  4434,  5450, 15871,   893,   229,    30, 10011,    57,\n",
      "          4098,   175, 26914,  1679,  9768,  1651, 26923,   251,  1475,  6679,\n",
      "          1346,  3602, 14610,  2000, 26914,  2019,   893,   229,  1340,     4],\n",
      "        [    3,   311,   128,  3107,   443,  9875,  6986, 18855,   355,   127,\n",
      "            39,  4496,   106,  2038,  4937, 24184,  2657, 26914,  1566, 13996,\n",
      "           246,    30,  9875, 13115, 26901,    27,  1641,   144,   149,     4],\n",
      "        [    3,  2579,   718,   181, 24651,    68,   167, 26935,   761, 26918,\n",
      "           557,   213,    39, 22821,  1068,  1754,    50,    86,  5214,   319,\n",
      "          2732,  2344,  8309,  6371, 10010,     7, 26914,  1679,   246,     4],\n",
      "        [    3,  9153,     6, 26918,  2633, 26918, 26239,     6, 26964,   291,\n",
      "           297,  3032,  4484,   267,  5964,   479,  2615,    39, 13424, 22983,\n",
      "             6,  4129,  3881,   266, 26914, 15209, 12247,  2112,   260,     4],\n",
      "        [    3,   917, 19609, 10710,    98,   114,   382, 26182, 26898,   127,\n",
      "           106,  8432,  1653,  3370,   198, 26935,  2250, 26935, 24617, 26914,\n",
      "          1882,   194,    30, 17905,   302, 11762,  2683, 26918,   246,     4],\n",
      "        [    3,   173,    86,  2213, 25235, 11152,  1075, 18072,   267, 26656,\n",
      "         15110, 26914,  2050, 20800,   389,    27, 24184,     7,  8137,   200,\n",
      "          1208,  1425, 26918,  6387,    81, 21495, 26914,     4,     0,     0],\n",
      "        [    3,  2250,  2025,   280,   287,    30, 22147, 10336,    91,  6080,\n",
      "            20, 26902, 26914,   912,  1828, 23371,   903,   213,   386,   356,\n",
      "         26918,   557,   213, 14622,  1962, 26902,   474, 26914,     4,     0],\n",
      "        [    3,  1993,   935,  6471,  4045, 10610, 13578,   127,   229,  2177,\n",
      "          1630,   142, 26864,    57, 13480, 26918, 25158,  4069,    81,  2239,\n",
      "         26914,  8137,  4522, 26964, 14510,   292,    93,  1868, 15717,     4],\n",
      "        [    3,  1292,  4359,  1315, 11540, 17625,   144,   115,    30,  2261,\n",
      "         25365,    21,  6499,   270,  6673, 26914, 11176,   127,  1868,    21,\n",
      "         11770,  2262, 26918,   167,  5261, 11440,    42,  6942,   115,     4]])\n",
      "torch.Size([32, 30])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([32, 30])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# have a look at one batch\n",
    "data = next(iter(dl_train))\n",
    "print(data.keys())\n",
    "input_ids = data['input_ids']\n",
    "print(input_ids)\n",
    "print(input_ids.shape)\n",
    "attention_mask = data['attention_mask']\n",
    "print(attention_mask)\n",
    "print(attention_mask.shape)\n",
    "print(data['target'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# das ist umständlich (und auch falsch), ich habe stattdessen bereits BertForSequenceClassification genommen\n",
    "# https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "\n",
    "#class Bert_regression(nn.Module):\n",
    "#    \n",
    "#    def __init__(self, n_outputs): # maybe train pageviews and timeOnPage simultaneously?\n",
    "#        super(Bert_regression, self).__init__()\n",
    "#        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "#        self.drop = nn.Dropout(p=0.3)\n",
    "#        self.out = nn.Linear(self.bert.config.hidden_size, n_outputs)\n",
    "#\n",
    "#    def forward(self, input_ids, attention_mask):\n",
    "#        _, pooled_output = self.bert(input_ids=input_ids,           # das hier ist glaube ich nicht sinnvoll bei mir\n",
    "#                                     attention_mask=attention_mask)\n",
    "#        output = self.drop(pooled_output)\n",
    "#   \n",
    "#        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Bert_regression(n_outputs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME,\n",
    "                                                      num_labels = 1, # turns \"classification\" into regression?\n",
    "                                                      output_attentions = False,\n",
    "                                                      output_hidden_states = False,\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0530],\n",
       "         [ 0.1250],\n",
       "         [ 0.0397],\n",
       "         [ 0.1117],\n",
       "         [ 0.2243],\n",
       "         [ 0.0653],\n",
       "         [ 0.1175],\n",
       "         [ 0.2201],\n",
       "         [ 0.1270],\n",
       "         [-0.0224],\n",
       "         [-0.1329],\n",
       "         [ 0.0188],\n",
       "         [ 0.0862],\n",
       "         [ 0.1502],\n",
       "         [ 0.0040],\n",
       "         [ 0.0829],\n",
       "         [ 0.0332],\n",
       "         [ 0.0683],\n",
       "         [-0.0279],\n",
       "         [-0.2214],\n",
       "         [ 0.0340],\n",
       "         [ 0.1542],\n",
       "         [ 0.0202],\n",
       "         [-0.0462],\n",
       "         [-0.0607],\n",
       "         [-0.0083],\n",
       "         [-0.0254],\n",
       "         [ 0.0145],\n",
       "         [ 0.0912],\n",
       "         [ 0.2070],\n",
       "         [ 0.2703],\n",
       "         [ 0.0668]], grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try ut with one batch (model is not trained yet so nothing exciting to be expected)\n",
    "model(input_ids, attention_mask) # semms good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.MSELoss()  # mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "training\n",
      "Mean train loss: 31169.826637268066\n",
      "evaluating\n",
      "Mean eval loss: 97320.0082397461\n",
      "Epoch 1\n",
      "training\n",
      "Mean train loss: 31157.769926651665\n",
      "evaluating\n",
      "Mean eval loss: 97300.71371459961\n",
      "Epoch 2\n",
      "training\n",
      "Mean train loss: 31151.131855840267\n",
      "evaluating\n",
      "Mean eval loss: 97283.52858479817\n",
      "Epoch 3\n",
      "training\n",
      "Mean train loss: 31142.364156971806\n",
      "evaluating\n",
      "Mean eval loss: 97267.3794555664\n",
      "Epoch 4\n",
      "training\n",
      "Mean train loss: 31131.699822135593\n",
      "evaluating\n",
      "Mean eval loss: 97252.37390136719\n",
      "Epoch 5\n",
      "training\n",
      "Mean train loss: 31122.820015948753\n",
      "evaluating\n",
      "Mean eval loss: 97237.1313273112\n",
      "Epoch 6\n",
      "training\n",
      "Mean train loss: 31115.45326664137\n",
      "evaluating\n",
      "Mean eval loss: 97224.20383707683\n",
      "Epoch 7\n",
      "training\n",
      "Mean train loss: 31111.353743511696\n",
      "evaluating\n",
      "Mean eval loss: 97210.33953857422\n",
      "Epoch 8\n",
      "training\n",
      "Mean train loss: 31098.77880859375\n",
      "evaluating\n",
      "Mean eval loss: 97197.90098063152\n",
      "Epoch 9\n",
      "training\n",
      "Mean train loss: 31093.607056327488\n",
      "evaluating\n",
      "Mean eval loss: 97185.94038899739\n",
      "Epoch 10\n",
      "training\n",
      "Mean train loss: 31088.732579770294\n",
      "evaluating\n",
      "Mean eval loss: 97173.45078531902\n",
      "Epoch 11\n",
      "training\n",
      "Mean train loss: 31081.165388687798\n",
      "evaluating\n",
      "Mean eval loss: 97162.265625\n",
      "Epoch 12\n",
      "training\n",
      "Mean train loss: 31072.285142649776\n",
      "evaluating\n",
      "Mean eval loss: 97151.23372395833\n",
      "Epoch 13\n",
      "training\n",
      "Mean train loss: 31065.161537502125\n",
      "evaluating\n",
      "Mean eval loss: 97140.49853515625\n",
      "Epoch 14\n",
      "training\n",
      "Mean train loss: 31061.764588729195\n",
      "evaluating\n",
      "Mean eval loss: 97130.44380696614\n",
      "Epoch 15\n",
      "training\n",
      "Mean train loss: 31059.355376202126\n",
      "evaluating\n",
      "Mean eval loss: 97120.09541829427\n",
      "Epoch 16\n",
      "training\n",
      "Mean train loss: 31051.070095891537\n",
      "evaluating\n",
      "Mean eval loss: 97110.2835591634\n",
      "Epoch 17\n",
      "training\n",
      "Mean train loss: 31048.112839408543\n",
      "evaluating\n",
      "Mean eval loss: 97101.10004679362\n",
      "Epoch 18\n",
      "training\n",
      "Mean train loss: 31048.42428058127\n",
      "evaluating\n",
      "Mean eval loss: 97091.55096435547\n",
      "Epoch 19\n",
      "training\n",
      "Mean train loss: 31034.3117695684\n",
      "evaluating\n",
      "Mean eval loss: 97082.5142211914\n",
      "Epoch 20\n",
      "training\n",
      "Mean train loss: 31031.61794679061\n",
      "evaluating\n",
      "Mean eval loss: 97073.39100138347\n",
      "Epoch 21\n",
      "training\n",
      "Mean train loss: 31029.644911724587\n",
      "evaluating\n",
      "Mean eval loss: 97064.70707194011\n",
      "Epoch 22\n",
      "training\n",
      "Mean train loss: 31028.10353818147\n",
      "evaluating\n",
      "Mean eval loss: 97056.12672932942\n",
      "Epoch 23\n",
      "training\n",
      "Mean train loss: 31015.814056396484\n",
      "evaluating\n",
      "Mean eval loss: 97047.69649251302\n",
      "Epoch 24\n",
      "training\n",
      "Mean train loss: 31020.48814624289\n",
      "evaluating\n",
      "Mean eval loss: 97039.59892781575\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    ### TRAINING on train\n",
    "    print(\"training\")\n",
    "    model = model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for nr, d in enumerate(dl_train):\n",
    "        print(\"-Batch\", nr, end='\\r')\n",
    "        input_ids = d[\"input_ids\"]\n",
    "        attention_mask = d[\"attention_mask\"]\n",
    "        targets = d[\"target\"]\n",
    "        #print(targets.shape)    \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)[0] # stimmt das so? ist [0] die logits?\n",
    "        #print(outputs.shape)\n",
    "        \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        #print(np.mean(train_losses))\n",
    "    print(\"Mean train loss:\", np.mean(train_losses))\n",
    "    \n",
    "    ### EVALUATING on dev\n",
    "    print(\"evaluating\")\n",
    "    model = model.eval()\n",
    "    eval_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for nr, d in enumerate(dl_dev):\n",
    "            print(\"-Batch\", nr, end='\\r')\n",
    "            input_ids = d[\"input_ids\"]\n",
    "            attention_mask = d[\"attention_mask\"]\n",
    "            targets = d[\"target\"]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)[0] # stimmt das so?\n",
    "        \n",
    "            loss = loss_fn(outputs, targets)\n",
    "            eval_losses.append(loss.item())\n",
    "            #print(np.mean(eval_losses))\n",
    "        print(\"Mean eval loss:\", np.mean(eval_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dl_dev_oneBatch, _ = create_DataLoaders(target = 'pageviews',\n",
    "                                               text_base = 'titelH1', # 'teaser'\n",
    "                                               tokenizer = tokenizer, \n",
    "                                               max_len = 20,            # change depending on used text_base!\n",
    "                                               batch_size = 89) # for now: just one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.2933, 12.3086, 12.3034, 12.2823, 12.2906, 12.2941, 12.3136, 12.2987,\n",
      "        12.2861, 12.2873, 12.2507, 12.2997, 12.2819, 12.2853, 12.3088, 12.2795,\n",
      "        12.2993, 12.2814, 12.2881, 12.3007, 12.2934, 12.2879, 12.2927, 12.2814,\n",
      "        12.2832, 12.3043, 12.3022, 12.2989, 12.2966, 12.3052, 12.2808, 12.3051,\n",
      "        12.2821, 12.2927, 12.2764, 12.2815, 12.2991, 12.2824, 12.2783, 12.2737,\n",
      "        12.2980, 12.3045, 12.2785, 12.2990, 12.2982, 12.3028, 12.2725, 12.2922,\n",
      "        12.2856, 12.2970, 12.2983, 12.2904, 12.2839, 12.2974, 12.2993, 12.2653,\n",
      "        12.3095, 12.2745, 12.2972, 12.2916, 12.3087, 12.2794, 12.2994, 12.2933,\n",
      "        12.2878, 12.3078, 12.2545, 12.2969, 12.2767, 12.2955, 12.3029, 12.3008,\n",
      "        12.2982, 12.2705, 12.2950, 12.2845, 12.2903, 12.3097, 12.3059, 12.2845,\n",
      "        12.2968, 12.2984, 12.3076, 12.3052, 12.2739, 12.2729, 12.2879, 12.3032,\n",
      "        12.2816])\n",
      "tensor([3.0470e+03, 3.3000e+01, 2.0000e+01, 6.4000e+01, 1.0000e+00, 1.2000e+01,\n",
      "        2.0000e+00, 2.2000e+01, 3.0000e+00, 6.0000e+00, 1.0000e+00, 1.9000e+01,\n",
      "        3.0000e+00, 2.6200e+02, 9.0000e+00, 4.0000e+00, 2.4000e+01, 1.0000e+02,\n",
      "        2.7000e+01, 1.2400e+02, 3.0000e+00, 8.0000e+00, 1.4000e+01, 6.0000e+00,\n",
      "        3.9000e+01, 1.4000e+01, 1.6000e+01, 1.8000e+01, 9.0000e+00, 8.0000e+00,\n",
      "        8.0000e+00, 9.0000e+00, 1.6000e+01, 3.2000e+01, 1.9000e+01, 8.0000e+00,\n",
      "        2.0000e+01, 3.0000e+00, 1.8000e+01, 1.1000e+01, 1.8000e+01, 9.0000e+00,\n",
      "        9.0000e+00, 2.4000e+01, 1.4000e+01, 6.0000e+00, 5.0000e+00, 1.1500e+02,\n",
      "        4.0000e+00, 1.4000e+01, 9.0000e+00, 7.0000e+00, 1.6000e+01, 1.7000e+01,\n",
      "        6.0000e+00, 8.0000e+00, 1.9000e+01, 3.9000e+01, 1.7000e+01, 2.9000e+01,\n",
      "        2.4000e+01, 1.3000e+01, 1.0000e+00, 8.0000e+00, 1.0000e+01, 3.0000e+00,\n",
      "        7.0000e+00, 1.4000e+01, 9.0000e+00, 7.0000e+00, 7.0000e+00, 2.0000e+00,\n",
      "        7.0000e+00, 4.0000e+00, 1.2000e+01, 5.0000e+00, 1.2000e+01, 7.0000e+00,\n",
      "        1.7300e+02, 1.5000e+01, 3.0000e+00, 1.8000e+01, 2.9000e+01, 2.1000e+01,\n",
      "        1.0000e+00, 1.0000e+01, 2.0000e+00, 9.0000e+00, 1.8000e+01])\n",
      "(0.022001277673926255, 0.8378454215220879)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for d in dl_dev_oneBatch:\n",
    "        pred_dev = model(input_ids=d[\"input_ids\"], attention_mask=d[\"attention_mask\"])[0] # just the logits?\n",
    "        y_dev = d[\"target\"]\n",
    "        \n",
    "        pred_dev = pred_dev.squeeze()\n",
    "        y_dev = y_dev.squeeze()\n",
    "        print(pred_dev)\n",
    "        print(y_dev)\n",
    "        print(st.pearsonr(pred_dev, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kommentare:\n",
    "* Das Modell lernt nicht! Liegt das einfach an der geringen Datenmenge oder ist im Training was falsch?\n",
    "* anderer Optimizer? Ist Loss richtig berechnet?\n",
    "* Oder liegt das Problem bereits beim DataSet, Dataloader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
