{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Sanna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "# download german fastText Embeddings\n",
    "import wget\n",
    "import gzip\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# downloading the .vec.gz-files (fasText, facebook)\n",
    "url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz'\n",
    "target_path = '/Users/Sanna/data/cc.de.300.vec.gz'\n",
    "if not os.path.isfile(target_path):\n",
    "    print('downloading...')\n",
    "    wget.download(url, out=target_path)\n",
    "    print('done')\n",
    "else:\n",
    "    print('file already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(path=None, limit=None):\n",
    "    print('loading embeddings ...')\n",
    "    f = gzip.open(path, 'rb')\n",
    "    n, d = map(int, f.readline().split())\n",
    "    data = {}\n",
    "    counter = 0\n",
    "    for line in f:\n",
    "        line = line.decode()\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(tokens[1:], dtype=np.float32)\n",
    "        if limit:\n",
    "            if counter >= limit:\n",
    "                break\n",
    "            else:\n",
    "                counter += 1\n",
    "    f.close()\n",
    "    print('done')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embeddings ...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "embs = load_vectors('/Users/Sanna/data/cc.de.300.vec.gz', limit = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[-0.016   0.0168  0.11    0.0636  0.0124]\n",
      "[-0.0561  0.0101  0.0784  0.0929  0.0192]\n",
      "[-0.0017  0.0986  0.1736  0.0391  0.0817]\n",
      "[ 0.017  -0.0775  0.012   0.0453 -0.1227]\n",
      "[0.0263 0.0028 0.0664 0.0028 0.0067]\n",
      "[-0.0354 -0.0088 -0.018   0.016  -0.0152]\n",
      "[ 0.0032  0.0169  0.0392 -0.1217 -0.0255]\n"
     ]
    }
   ],
   "source": [
    "# exploring the embeddings to see what the preprocessing should be like?\n",
    "print(embs.get('Garten').shape)\n",
    "print(embs['das'][:5])\n",
    "print(embs['Das'][:5])\n",
    "print(embs['ging'][:5])\n",
    "print(embs['gehst'][:5])\n",
    "print(embs['gegangen'][:5])\n",
    "print(embs[','][:5])\n",
    "print(embs['corona'][:5]) # :D Tja, das ist wohl eher italienisch oder Bier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path('/Volumes/INWT/Daten_NLP/') # encrypted folder!\n",
    "DATA = ROOT / '200707_aachener_zeitung_modified.csv' # contains new columns and (minimal) preprocessed texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageviews</th>\n",
       "      <th>entrances</th>\n",
       "      <th>exits</th>\n",
       "      <th>bounces</th>\n",
       "      <th>timeOnPage</th>\n",
       "      <th>conversions</th>\n",
       "      <th>avgTimeOnPage</th>\n",
       "      <th>stickiness</th>\n",
       "      <th>entranceRate</th>\n",
       "      <th>bounceRate</th>\n",
       "      <th>...</th>\n",
       "      <th>titelH3</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>category</th>\n",
       "      <th>city</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>avgTimeOnPage/wordcount</th>\n",
       "      <th>nr_tokens</th>\n",
       "      <th>mean_token_length</th>\n",
       "      <th>nr_tokens_teaser</th>\n",
       "      <th>nr_tokens_titelH1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>articleId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48620281</th>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1012</td>\n",
       "      <td></td>\n",
       "      <td>112.444444</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>769</td>\n",
       "      <td>vm</td>\n",
       "      <td>München/Stuttgart</td>\n",
       "      <td>Frische Luft und Bewegung: Diese Kombination r...</td>\n",
       "      <td>0.146222</td>\n",
       "      <td>796</td>\n",
       "      <td>5.359296</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48620381</th>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1484</td>\n",
       "      <td></td>\n",
       "      <td>185.500000</td>\n",
       "      <td>42.105263</td>\n",
       "      <td>31.578947</td>\n",
       "      <td>26.315789</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>441</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin/Frankfurt/Main</td>\n",
       "      <td>Der Wecker klingelt, aufstehen! Doch gerade im...</td>\n",
       "      <td>0.420635</td>\n",
       "      <td>452</td>\n",
       "      <td>5.938053</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48622639</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>390</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Eltern auf der Suche nach einem guten Babyphon...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>396</td>\n",
       "      <td>5.848485</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48623085</th>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>974</td>\n",
       "      <td></td>\n",
       "      <td>81.166667</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>28.125000</td>\n",
       "      <td>28.125000</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>345</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Spülmaschinentabs sollen kleine Alleskönner se...</td>\n",
       "      <td>0.235266</td>\n",
       "      <td>367</td>\n",
       "      <td>5.594005</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48623259</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3797</td>\n",
       "      <td></td>\n",
       "      <td>223.352941</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>182</td>\n",
       "      <td>vm</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Make-up hat heutzutage einen Zweck: Es soll da...</td>\n",
       "      <td>1.227214</td>\n",
       "      <td>183</td>\n",
       "      <td>5.622951</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pageviews  entrances  exits  bounces  timeOnPage conversions  \\\n",
       "articleId                                                                 \n",
       "48620281          21          7     12        7        1012               \n",
       "48620381          19          6     11        5        1484               \n",
       "48622639           2          2      2        2           0               \n",
       "48623085          32          9     20        9         974               \n",
       "48623259          24          2      7        2        3797               \n",
       "\n",
       "           avgTimeOnPage  stickiness  entranceRate  bounceRate  ... titelH3  \\\n",
       "articleId                                                       ...           \n",
       "48620281      112.444444   42.857143     33.333333   33.333333  ...           \n",
       "48620381      185.500000   42.105263     31.578947   26.315789  ...           \n",
       "48622639        0.000000    0.000000    100.000000  100.000000  ...           \n",
       "48623085       81.166667   37.500000     28.125000   28.125000  ...           \n",
       "48623259      223.352941   70.833333      8.333333    8.333333  ...           \n",
       "\n",
       "          wordcount category                   city  \\\n",
       "articleId                                             \n",
       "48620281        769       vm      München/Stuttgart   \n",
       "48620381        441       vm  Berlin/Frankfurt/Main   \n",
       "48622639        390       vm                 Berlin   \n",
       "48623085        345       vm                 Berlin   \n",
       "48623259        182       vm                 Berlin   \n",
       "\n",
       "                                           text_preprocessed  \\\n",
       "articleId                                                      \n",
       "48620281   Frische Luft und Bewegung: Diese Kombination r...   \n",
       "48620381   Der Wecker klingelt, aufstehen! Doch gerade im...   \n",
       "48622639   Eltern auf der Suche nach einem guten Babyphon...   \n",
       "48623085   Spülmaschinentabs sollen kleine Alleskönner se...   \n",
       "48623259   Make-up hat heutzutage einen Zweck: Es soll da...   \n",
       "\n",
       "          avgTimeOnPage/wordcount nr_tokens mean_token_length  \\\n",
       "articleId                                                       \n",
       "48620281                 0.146222       796          5.359296   \n",
       "48620381                 0.420635       452          5.938053   \n",
       "48622639                 0.000000       396          5.848485   \n",
       "48623085                 0.235266       367          5.594005   \n",
       "48623259                 1.227214       183          5.622951   \n",
       "\n",
       "          nr_tokens_teaser nr_tokens_titelH1  \n",
       "articleId                                     \n",
       "48620281                29                 9  \n",
       "48620381                33                 8  \n",
       "48622639                30                 7  \n",
       "48623085                30                 7  \n",
       "48623259                22                 8  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA, index_col = 0)\n",
    "df = df.fillna('') # replacing Nan with emtpy string\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self, delete_stopwords=False, lemmatize=False, delete_punctuation=False):\n",
    "        self.nlp = spacy.load(\"de_core_news_sm\", disable=['parser', 'ner'])\n",
    "        #self.nlp = spacy.load(\"de_core_news_md\", disable=['parser', 'ner'])\n",
    "        self.delete_stopwords = delete_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.delete_punctuation = delete_punctuation\n",
    "        self.stopwords = nltk.corpus.stopwords.words('german')\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        rt = []\n",
    "        doc = self.nlp(doc)\n",
    "        \n",
    "        if self.lemmatize==True:\n",
    "            for token in doc:\n",
    "                rt.append(token.lemma_.lower())\n",
    "        else:\n",
    "            for token in doc:\n",
    "                rt.append(token.text)\n",
    "\n",
    "        if self.delete_punctuation==True:\n",
    "            rt = [ t for t in rt if t not in string.punctuation ]\n",
    "        \n",
    "        if self.delete_stopwords==True:\n",
    "            if self.lemmatize == True:\n",
    "                self.stopwords = [ self.nlp(s)[0].lemma_ for s in self.stopwords ]\n",
    "            rt = [ t for t in rt if t not in self.stopwords ]\n",
    "\n",
    "        return \" \".join(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das hier sind verschiedene Wörter , um alle Dinge zu testen und anzuschauen .\n",
      "Das verschiedene Wörter , Dinge testen anzuschauen .\n",
      "Das hier sind verschiedene Wörter um alle Dinge zu testen und anzuschauen\n",
      "der hier sein verschieden wort um all ding zu testen und anschauen\n",
      "verschieden wort ding testen anschauen\n"
     ]
    }
   ],
   "source": [
    "s = \"Das hier sind verschiedene Wörter, um alle Dinge zu testen und anzuschauen.\"\n",
    "pr1 = Preprocessor()\n",
    "print(pr1(s))\n",
    "pr2 = Preprocessor(delete_stopwords=True)\n",
    "print(pr2(s))\n",
    "pr3 = Preprocessor(delete_punctuation=True)\n",
    "print(pr3(s))\n",
    "pr4 = Preprocessor(lemmatize=True, delete_punctuation=True)\n",
    "print(pr4(s))\n",
    "pr5 = Preprocessor(lemmatize=True, delete_stopwords=True, delete_punctuation=True)\n",
    "print(pr5(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_embs(text, preprocessor, embs):\n",
    "    vector = np.zeros(300)\n",
    "    text_preprocessed = preprocessor(text)\n",
    "    tokens = text_preprocessed.split() # preprocessor returns string with \" \" as seperator, so needs to be split up\n",
    "    counter = 0\n",
    "    for t in tokens:\n",
    "        #print(t)\n",
    "        if t in embs:\n",
    "            vector += embs.get(t)\n",
    "            counter +=1\n",
    "    #print(counter)\n",
    "    if counter !=0:\n",
    "        vector = vector/counter\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepr = Preprocessor(lemmatize=True, delete_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = get_averaged_embs(\"Nicht jeder geht gerne ins Schwimmbad, nur weil die Sonne scheint.\",\n",
    "                            preprocessor = prepr,\n",
    "                            embs = embs)\n",
    "example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 35) (89, 35) (90, 35)\n"
     ]
    }
   ],
   "source": [
    "#creating train, dev, test\n",
    "RANDOM_SEED = 123\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_dev, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED, shuffle=True)\n",
    "print(df_train.shape, df_dev.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features: averaged embeddings of the text (or teaser, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define text base features: text? text_preprocessed? titelH1?\n",
    "\n",
    "feature = 'text_preprocessed'\n",
    "#feature = 'titelH1'\n",
    "#feature = 'teaser' # note: not all have 'teaser' but I replaced Nan wih empty string \"\"\n",
    "\n",
    "X_train = np.array([ get_averaged_embs(text, preprocessor=prepr, embs=embs) for text in df_train[feature] ])\n",
    "X_dev = np.array([ get_averaged_embs(text, preprocessor=prepr, embs=embs) for text in df_dev[feature] ])\n",
    "X_test = np.array([ get_averaged_embs(text, preprocessor=prepr, embs=embs) for text in df_test[feature] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712, 300), (89, 300), (90, 300))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_dev.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the target labels\n",
    "#target = 'timeOnPage'\n",
    "target =  'pageviews'\n",
    "#target = 'avgTimeOnPage'\n",
    "#target = 'stickiness'\n",
    "\n",
    "y_train = np.array(df_train[target])\n",
    "y_dev = np.array(df_dev[target])\n",
    "y_test = np.array(df_test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712,), (89,), (90,))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_dev.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose model ???\n",
    "model = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict for dev set\n",
    "pred_dev = model.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66.09238913, 19.7963544 ,  5.31039472, 33.18575782, 20.23860999,\n",
       "       29.9962147 , 29.44286806, 28.2507586 , 10.32347411, 50.4317532 ,\n",
       "       48.53036061, 47.62864136,  9.68556234, 14.37338586, 41.17240008,\n",
       "       35.72628765, 43.01535499, 63.12380705, 22.78533407, 71.35635256,\n",
       "       71.19728059, 29.39557016, 46.30525339, 44.97687027, 60.93388843,\n",
       "       37.92476161, 31.4556853 , 55.97063217, 42.11671319, 49.22450772,\n",
       "       50.73903052, 28.81379404, 61.3329802 , 25.59293726, 69.304076  ,\n",
       "       36.96100997, 32.91148499,  6.08442219, 28.47456071, 56.44512564,\n",
       "       41.35323922, 39.39745109, 11.14293029, 28.1758878 ,  9.60988004,\n",
       "       38.9900646 , 46.18551785, 58.12965785, 20.22825571, 22.85680856,\n",
       "       48.76808666, 44.00123859, 20.12032599, 16.21154834, 66.24928451,\n",
       "       34.12627147, 50.60897622, 33.79176813, 39.70805523, 20.34371296,\n",
       "       29.12702502, 56.94202829, 30.21366103, 22.4359978 , 16.62886993,\n",
       "       38.97272018, 14.02136826, 57.46088121, 48.23670629, 26.06345   ,\n",
       "        4.69598293, 35.9435701 , 38.06657918, 69.86179166, 30.10517779,\n",
       "       49.91571185, 21.52823632, 49.12190265, 40.79489574, 22.06302116,\n",
       "       35.48559393, 24.54329858, 27.32600426, 28.51407787, 23.28212301,\n",
       "        5.39187501, 14.47499934, 19.61025605, 37.11118959])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# postprocessing: replace negative values with 0 (better way? can I give that hint to the model?)\n",
    "pred_dev[pred_dev < 0] = 0\n",
    "pred_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3047,   33,   20,   64,    1,   12,    2,   22,    3,    6,    1,\n",
       "         19,    3,  262,    9,    4,   24,  100,   27,  124,    3,    8,\n",
       "         14,    6,   39,   14,   16,   18,    9,    8,    8,    9,   16,\n",
       "         32,   19,    8,   20,    3,   18,   11,   18,    9,    9,   24,\n",
       "         14,    6,    5,  115,    4,   14,    9,    7,   16,   17,    6,\n",
       "          8,   19,   39,   17,   29,   24,   13,    1,    8,   10,    3,\n",
       "          7,   14,    9,    7,    7,    2,    7,    4,   12,    5,   12,\n",
       "          7,  173,   15,    3,   18,   29,   21,    1,   10,    2,    9,\n",
       "         18])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.20071014672268453, 0.059299224221468286)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.pearsonr(pred_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notizen:\n",
    "\n",
    "Keine so gute Evaluation, woran könnte das liegen? Mitteln der Embeddings einfach nicht sinnvoll? Präprozessierung ungeeignet für die Embeddings (Kleinschreibung, Lemmatisierung etc.)?\n",
    "\n",
    "Sowohl bei pageviews als auch timeOnPage und auch bei Text/Teaser/Titel ähnlich schlecht.\n",
    "\n",
    "Feature-Berechnung dauert sehr lange, kann man das irgendwie schlauer machen und beschleunigen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
