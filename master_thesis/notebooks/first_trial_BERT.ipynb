{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from master_thesis.src import utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-german-cased' # 'distilbert-base-german-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Das', 'hier', 'ist', 'ein', 'deutscher', 'Beispiel', '##text', '.', 'Und', 'einen', 'zweiten', 'müssen', 'wir', 'auch', 'noch', 'haben', '.']\n",
      "[295, 702, 127, 39, 2433, 2249, 8859, 26914, 1356, 303, 1909, 1475, 232, 194, 357, 474, 26914]\n",
      "[3, 295, 702, 127, 39, 2433, 2249, 8859, 26914, 1356, 303, 1909, 1475, 232, 194, 357, 474, 26914, 4]\n",
      "---\n",
      "{'input_ids': [3, 295, 702, 127, 39, 2433, 2249, 8859, 26914, 4], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Das hier ist ein deutscher Beispieltext. Und einen zweiten müssen wir auch noch haben.\"\n",
    "tokens = tokenizer.tokenize(sample_text) # just tokenizes\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids = tokenizer.encode(sample_text) # already adds special tokens\n",
    "encoded_plus = tokenizer.encode_plus(sample_text,\n",
    "                                     max_length = 10,\n",
    "                                     return_token_type_ids=False,\n",
    "                                     pad_to_max_length=True,\n",
    "                                     return_attention_mask=True,)\n",
    "\n",
    "print(tokens)\n",
    "print(token_ids)\n",
    "print(ids)\n",
    "print(\"---\")\n",
    "print(encoded_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.get_vocab() # shows tokenizer vocab (subwords!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[SEP]', 4, '[CLS]', 3, '[PAD]', 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token, tokenizer.sep_token_id, tokenizer.cls_token, tokenizer.cls_token_id, tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROOT = Path('/Volumes/INWT/Daten_NLP/') # local (Laptop)\n",
    "#ROOT = Path('/home/ruecker/data/Daten_INWT/') # JULIE-Server\n",
    "\n",
    "#DATA = ROOT / '200707_aachener_zeitung_modified.csv' # text is already minimal preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class INWT_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, target, text_base, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.text_base = text_base\n",
    "        self.target = target\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.df.loc[item, self.text_base])\n",
    "        target = np.array(self.df.loc[item, self.target])\n",
    "\n",
    "        # hier einfach encode() nehmen? brauche ich die attention_mask etc?\n",
    "        encoding = self.tokenizer.encode_plus(text,\n",
    "                                              max_length=self.max_len,\n",
    "                                              truncation=True,\n",
    "                                              return_token_type_ids=False,\n",
    "                                              pad_to_max_length=True,\n",
    "                                              return_attention_mask=True,\n",
    "                                              return_tensors='pt',\n",
    "                                              )\n",
    "\n",
    "        return {'text': text,\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'target': torch.tensor(target, dtype=torch.float).unsqueeze(dim=-1) # unsqueezing so shape (batch_size,1)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared columns: {'dpaGuid', 'avgTimeOnPage', 'bounces', 'avgTimeOnPagePerWordcount', 'pageviews_percentile', 'titel', 'nr_tokens_text', 'zeilen', 'nr_tokens_publisher', 'prozentVerlag', 'entrances', 'article_text', 'pagePath', 'avgTimeOnPage_percentile', 'prozentDpa', 'timeOnPage', 'pageviews', 'category', 'rubric', 'publisher', 'exits', 'date'}\n",
      "Shape of raw df: (92020, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dpaGuid</th>\n",
       "      <th>avgTimeOnPage</th>\n",
       "      <th>bounces</th>\n",
       "      <th>avgTimeOnPagePerWordcount</th>\n",
       "      <th>pageviews_percentile</th>\n",
       "      <th>titel</th>\n",
       "      <th>nr_tokens_text</th>\n",
       "      <th>zeilen</th>\n",
       "      <th>nr_tokens_publisher</th>\n",
       "      <th>prozentVerlag</th>\n",
       "      <th>...</th>\n",
       "      <th>pagePath</th>\n",
       "      <th>avgTimeOnPage_percentile</th>\n",
       "      <th>prozentDpa</th>\n",
       "      <th>timeOnPage</th>\n",
       "      <th>pageviews</th>\n",
       "      <th>category</th>\n",
       "      <th>rubric</th>\n",
       "      <th>publisher</th>\n",
       "      <th>exits</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>articleId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SZ_16591</th>\n",
       "      <td>NaN</td>\n",
       "      <td>47.587949</td>\n",
       "      <td>76</td>\n",
       "      <td>0.088949</td>\n",
       "      <td>90.448376</td>\n",
       "      <td>Sparbuch der Kinder ist tabu : Urteil: Vater d...</td>\n",
       "      <td>535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.993679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48968.0</td>\n",
       "      <td>2411</td>\n",
       "      <td>sz-spezial</td>\n",
       "      <td>recht</td>\n",
       "      <td>SZ</td>\n",
       "      <td>1382</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SZ_16595</th>\n",
       "      <td>NaN</td>\n",
       "      <td>40.826923</td>\n",
       "      <td>6</td>\n",
       "      <td>0.163308</td>\n",
       "      <td>3.960089</td>\n",
       "      <td>Gewalt in der Schule : Faustschlag ins Gesicht...</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.765865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2123.0</td>\n",
       "      <td>92</td>\n",
       "      <td>sz-spezial</td>\n",
       "      <td>recht</td>\n",
       "      <td>SZ</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SZ_16723</th>\n",
       "      <td>NaN</td>\n",
       "      <td>38.501931</td>\n",
       "      <td>26</td>\n",
       "      <td>0.166675</td>\n",
       "      <td>70.071579</td>\n",
       "      <td>Abflussrohr der Dachrinne defekt: Gebäudeversi...</td>\n",
       "      <td>231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.811973</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9972.0</td>\n",
       "      <td>567</td>\n",
       "      <td>sz-spezial</td>\n",
       "      <td>recht</td>\n",
       "      <td>SZ</td>\n",
       "      <td>308</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SZ_17146</th>\n",
       "      <td>NaN</td>\n",
       "      <td>98.246154</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125154</td>\n",
       "      <td>6.789167</td>\n",
       "      <td>„Er war die Liebe meines Lebens“</td>\n",
       "      <td>785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.367253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6386.0</td>\n",
       "      <td>101</td>\n",
       "      <td>magazine</td>\n",
       "      <td>momente</td>\n",
       "      <td>SZ</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SZ_17184</th>\n",
       "      <td>NaN</td>\n",
       "      <td>70.817518</td>\n",
       "      <td>9</td>\n",
       "      <td>0.177933</td>\n",
       "      <td>49.028570</td>\n",
       "      <td>Neuer Laptop kaputt - Rücktritt vom Kauf möglich?</td>\n",
       "      <td>398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.262519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9702.0</td>\n",
       "      <td>261</td>\n",
       "      <td>sz-spezial</td>\n",
       "      <td>recht</td>\n",
       "      <td>SZ</td>\n",
       "      <td>124</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          dpaGuid  avgTimeOnPage  bounces  avgTimeOnPagePerWordcount  \\\n",
       "articleId                                                              \n",
       "SZ_16591      NaN      47.587949       76                   0.088949   \n",
       "SZ_16595      NaN      40.826923        6                   0.163308   \n",
       "SZ_16723      NaN      38.501931       26                   0.166675   \n",
       "SZ_17146      NaN      98.246154        1                   0.125154   \n",
       "SZ_17184      NaN      70.817518        9                   0.177933   \n",
       "\n",
       "           pageviews_percentile  \\\n",
       "articleId                         \n",
       "SZ_16591              90.448376   \n",
       "SZ_16595               3.960089   \n",
       "SZ_16723              70.071579   \n",
       "SZ_17146               6.789167   \n",
       "SZ_17184              49.028570   \n",
       "\n",
       "                                                       titel  nr_tokens_text  \\\n",
       "articleId                                                                      \n",
       "SZ_16591   Sparbuch der Kinder ist tabu : Urteil: Vater d...             535   \n",
       "SZ_16595   Gewalt in der Schule : Faustschlag ins Gesicht...             250   \n",
       "SZ_16723   Abflussrohr der Dachrinne defekt: Gebäudeversi...             231   \n",
       "SZ_17146                    „Er war die Liebe meines Lebens“             785   \n",
       "SZ_17184   Neuer Laptop kaputt - Rücktritt vom Kauf möglich?             398   \n",
       "\n",
       "           zeilen  nr_tokens_publisher  prozentVerlag  ...  pagePath  \\\n",
       "articleId                                              ...             \n",
       "SZ_16591      NaN                  535            NaN  ...       NaN   \n",
       "SZ_16595      NaN                  250            NaN  ...       NaN   \n",
       "SZ_16723      NaN                  231            NaN  ...       NaN   \n",
       "SZ_17146      NaN                  785            NaN  ...       NaN   \n",
       "SZ_17184      NaN                  398            NaN  ...       NaN   \n",
       "\n",
       "          avgTimeOnPage_percentile prozentDpa  timeOnPage  pageviews  \\\n",
       "articleId                                                              \n",
       "SZ_16591                 56.993679        NaN     48968.0       2411   \n",
       "SZ_16595                 47.765865        NaN      2123.0         92   \n",
       "SZ_16723                 43.811973        NaN      9972.0        567   \n",
       "SZ_17146                 84.367253        NaN      6386.0        101   \n",
       "SZ_17184                 74.262519        NaN      9702.0        261   \n",
       "\n",
       "             category   rubric publisher exits date  \n",
       "articleId                                            \n",
       "SZ_16591   sz-spezial    recht        SZ  1382  NaN  \n",
       "SZ_16595   sz-spezial    recht        SZ    40  NaN  \n",
       "SZ_16723   sz-spezial    recht        SZ   308  NaN  \n",
       "SZ_17146     magazine  momente        SZ    36  NaN  \n",
       "SZ_17184   sz-spezial    recht        SZ   124  NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = utils.get_raw_df()\n",
    "#df_raw = df_raw.fillna('') # replacing Nan with emtpy string\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3826\n"
     ]
    }
   ],
   "source": [
    "df = df_raw\n",
    "df = df[df.publisher == \"NOZ\"]\n",
    "df = df.sample(frac=0.1, replace=False, random_state=1) # take 20% for faster processing # TODO: change back\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3060, 22) (383, 22) (383, 22)\n"
     ]
    }
   ],
   "source": [
    "#creating train, dev, test\n",
    "RANDOM_SEED = 123\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_dev, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED, shuffle=True)\n",
    "df_train.reset_index(drop=True, inplace=True) # so that index starts with 0 again\n",
    "df_dev.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "print(df_train.shape, df_dev.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dpaGuid</th>\n",
       "      <th>avgTimeOnPage</th>\n",
       "      <th>bounces</th>\n",
       "      <th>avgTimeOnPagePerWordcount</th>\n",
       "      <th>pageviews_percentile</th>\n",
       "      <th>titel</th>\n",
       "      <th>nr_tokens_text</th>\n",
       "      <th>zeilen</th>\n",
       "      <th>nr_tokens_publisher</th>\n",
       "      <th>prozentVerlag</th>\n",
       "      <th>...</th>\n",
       "      <th>pagePath</th>\n",
       "      <th>avgTimeOnPage_percentile</th>\n",
       "      <th>prozentDpa</th>\n",
       "      <th>timeOnPage</th>\n",
       "      <th>pageviews</th>\n",
       "      <th>category</th>\n",
       "      <th>rubric</th>\n",
       "      <th>publisher</th>\n",
       "      <th>exits</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>169.016973</td>\n",
       "      <td>2948</td>\n",
       "      <td>0.433377</td>\n",
       "      <td>94.172121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.152259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119495.0</td>\n",
       "      <td>4921</td>\n",
       "      <td>lokales</td>\n",
       "      <td>werlte</td>\n",
       "      <td>NOZ</td>\n",
       "      <td>4214</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191219-99-211934</td>\n",
       "      <td>136.306452</td>\n",
       "      <td>3</td>\n",
       "      <td>0.411802</td>\n",
       "      <td>4.195073</td>\n",
       "      <td>\": Demokrat wechselt zu Trumps\\nRepublikanern</td>\n",
       "      <td>331</td>\n",
       "      <td>36.0</td>\n",
       "      <td>331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.noz.de/deutschland-welt/politik/ar...</td>\n",
       "      <td>28.956459</td>\n",
       "      <td>96.0</td>\n",
       "      <td>8451.0</td>\n",
       "      <td>100</td>\n",
       "      <td>deutschland-und-welt</td>\n",
       "      <td>politik</td>\n",
       "      <td>NOZ</td>\n",
       "      <td>38</td>\n",
       "      <td>20191220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>127.804762</td>\n",
       "      <td>44</td>\n",
       "      <td>0.383798</td>\n",
       "      <td>46.674166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.125080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26839.0</td>\n",
       "      <td>362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.noz.de</td>\n",
       "      <td>NOZ</td>\n",
       "      <td>152</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>164.671674</td>\n",
       "      <td>292</td>\n",
       "      <td>0.475930</td>\n",
       "      <td>75.670394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.288883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76737.0</td>\n",
       "      <td>1169</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.noz.de</td>\n",
       "      <td>NOZ</td>\n",
       "      <td>703</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>575.760000</td>\n",
       "      <td>514</td>\n",
       "      <td>0.618432</td>\n",
       "      <td>75.179529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.259249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71970.0</td>\n",
       "      <td>1136</td>\n",
       "      <td>deutschland-und-welt</td>\n",
       "      <td>gut-zu-wissen</td>\n",
       "      <td>NOZ</td>\n",
       "      <td>1011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dpaGuid  avgTimeOnPage  bounces  avgTimeOnPagePerWordcount  \\\n",
       "0               NaN     169.016973     2948                   0.433377   \n",
       "1  191219-99-211934     136.306452        3                   0.411802   \n",
       "2               NaN     127.804762       44                   0.383798   \n",
       "3               NaN     164.671674      292                   0.475930   \n",
       "4               NaN     575.760000      514                   0.618432   \n",
       "\n",
       "   pageviews_percentile                                          titel  \\\n",
       "0             94.172121                                            NaN   \n",
       "1              4.195073  \": Demokrat wechselt zu Trumps\\nRepublikanern   \n",
       "2             46.674166                                            NaN   \n",
       "3             75.670394                                            NaN   \n",
       "4             75.179529                                            NaN   \n",
       "\n",
       "   nr_tokens_text  zeilen  nr_tokens_publisher  prozentVerlag  ...  \\\n",
       "0             390     NaN                  390            NaN  ...   \n",
       "1             331    36.0                  331            NaN  ...   \n",
       "2             333     NaN                  333            NaN  ...   \n",
       "3             346     NaN                  346            NaN  ...   \n",
       "4             931     NaN                  931            NaN  ...   \n",
       "\n",
       "                                            pagePath avgTimeOnPage_percentile  \\\n",
       "0                                                NaN                50.152259   \n",
       "1  https://www.noz.de/deutschland-welt/politik/ar...                28.956459   \n",
       "2                                                NaN                24.125080   \n",
       "3                                                NaN                47.288883   \n",
       "4                                                NaN                98.259249   \n",
       "\n",
       "  prozentDpa  timeOnPage  pageviews              category         rubric  \\\n",
       "0        NaN    119495.0       4921               lokales         werlte   \n",
       "1       96.0      8451.0        100  deutschland-und-welt        politik   \n",
       "2        NaN     26839.0        362                   NaN     www.noz.de   \n",
       "3        NaN     76737.0       1169                   NaN     www.noz.de   \n",
       "4        NaN     71970.0       1136  deutschland-und-welt  gut-zu-wissen   \n",
       "\n",
       "  publisher exits        date  \n",
       "0       NOZ  4214         NaN  \n",
       "1       NOZ    38  20191220.0  \n",
       "2       NOZ   152         NaN  \n",
       "3       NOZ   703         NaN  \n",
       "4       NOZ  1011         NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_DataLoaders(target, text_base, tokenizer, max_len, batch_size):\n",
    "    # creating DataSets\n",
    "    ds_train = INWT_Dataset(df=df_train,\n",
    "                  target = target,\n",
    "                  text_base = text_base,\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_len = max_len)\n",
    "    ds_dev = INWT_Dataset(df=df_dev,\n",
    "                  target = target,\n",
    "                  text_base = text_base,\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_len = max_len)\n",
    "    ds_test = INWT_Dataset(df=df_test,\n",
    "                  target = target,\n",
    "                  text_base = text_base,\n",
    "                  tokenizer=tokenizer,\n",
    "                  max_len = max_len)\n",
    "    \n",
    "    # creating DataLoaders\n",
    "    dl_train = DataLoader(ds_train, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "    dl_dev = DataLoader(ds_dev, batch_size=batch_size, num_workers=4)\n",
    "    dl_test = DataLoader(ds_test, batch_size=batch_size, num_workers=4)\n",
    "    \n",
    "    return dl_train, dl_dev, dl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train, dl_dev, dl_test = create_DataLoaders(target = 'avgTimeOnPage',\n",
    "                                               text_base = 'article_text', # 'titelH1',\n",
    "                                               tokenizer = tokenizer, \n",
    "                                               max_len = 300,            # change depending on used text_base!\n",
    "                                               batch_size = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'input_ids', 'attention_mask', 'target'])\n",
      "torch.Size([12, 300])\n",
      "torch.Size([12, 300])\n",
      "torch.Size([12, 1])\n"
     ]
    }
   ],
   "source": [
    "# have a look at one batch\n",
    "data = next(iter(dl_train))\n",
    "print(data.keys())\n",
    "input_ids = data['input_ids']\n",
    "#print(input_ids)\n",
    "print(input_ids.shape)\n",
    "attention_mask = data['attention_mask']\n",
    "#print(attention_mask)\n",
    "print(attention_mask.shape)\n",
    "print(data['target'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# das ist umständlich (und auch falsch), ich habe stattdessen bereits BertForSequenceClassification genommen\n",
    "# https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "\n",
    "#class Bert_regression(nn.Module):\n",
    "#    \n",
    "#    def __init__(self, n_outputs): # maybe train pageviews and timeOnPage simultaneously?\n",
    "#        super(Bert_regression, self).__init__()\n",
    "#        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "#        self.drop = nn.Dropout(p=0.3)\n",
    "#        self.out = nn.Linear(self.bert.config.hidden_size, n_outputs)\n",
    "#\n",
    "#    def forward(self, input_ids, attention_mask):\n",
    "#        _, pooled_output = self.bert(input_ids=input_ids,           # das hier ist glaube ich nicht sinnvoll bei mir\n",
    "#                                     attention_mask=attention_mask)\n",
    "#        output = self.drop(pooled_output)        \n",
    "#        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Bert_regression(n_outputs = 1)\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME,\n",
    "                                                      num_labels = 1, # turns \"classification\" into regression?\n",
    "                                                      output_attentions = False,\n",
    "                                                      output_hidden_states = False,\n",
    "                                                      )\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3009],\n",
       "         [-0.0678],\n",
       "         [-0.2119],\n",
       "         [-0.2000],\n",
       "         [-0.4979],\n",
       "         [-0.1847],\n",
       "         [-0.1303],\n",
       "         [-0.0585],\n",
       "         [-0.2620],\n",
       "         [ 0.2044],\n",
       "         [-0.2960],\n",
       "         [-0.2276]], grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try ut with one batch (model is not trained yet so nothing exciting to be expected)\n",
    "model(input_ids.to(device), attention_mask.to(device)) # semms good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.MSELoss()  # mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "training\n",
      "-Batch 249\r"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch\", epoch)\n",
    "    \n",
    "    ### TRAINING on train\n",
    "    print(\"training\")\n",
    "    model = model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for nr, d in enumerate(dl_train):\n",
    "        print(\"-Batch\", nr, end='\\r')\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"target\"].to(device)\n",
    "        #print(targets.shape)    \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)[0] # stimmt das so? ist [0] die logits?\n",
    "        #print(outputs.shape)\n",
    "        \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        #print(np.mean(train_losses))\n",
    "    print(\"Mean train loss:\", np.mean(train_losses))\n",
    "    \n",
    "    ### EVALUATING on dev\n",
    "    print(\"evaluating\")\n",
    "    model = model.eval()\n",
    "    eval_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for nr, d in enumerate(dl_dev):\n",
    "            print(\"-Batch\", nr, end='\\r')\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"target\"].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)[0] # stimmt das so?\n",
    "        \n",
    "            loss = loss_fn(outputs, targets)\n",
    "            eval_losses.append(loss.item())\n",
    "            #print(np.mean(eval_losses))\n",
    "        print(\"Mean eval loss:\", np.mean(eval_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dl_dev_oneBatch, _ = create_DataLoaders(target = 'avgTimeOnPage/wordcount',\n",
    "                                               text_base = 'text_preprocessed', # 'teaser'\n",
    "                                               tokenizer = tokenizer, \n",
    "                                               max_len = 100,            # change depending on used text_base!\n",
    "                                               batch_size = 89) # for now: just one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5329,  0.2562,  0.4639,  0.3975,  0.3378,  0.1366,  0.3866,  0.5034,\n",
      "         0.4795,  0.3761,  0.3445,  0.4251,  0.8253,  0.2645,  0.3172,  0.6630,\n",
      "         0.3116,  0.1757,  0.4417,  0.0116,  0.5449,  0.9076,  0.1101,  0.7867,\n",
      "         1.0569,  0.3484,  0.5437,  0.1672,  0.3955,  0.1168,  0.7323,  0.6364,\n",
      "         0.1067,  0.4060,  0.7389,  0.3821,  0.5545,  0.4256,  0.4132,  1.2090,\n",
      "         0.2423,  0.3605, -0.0208,  0.1978,  0.2604,  0.3660,  0.2688,  0.3128,\n",
      "         0.3575,  0.2891,  0.1810,  0.1984,  0.1762,  0.3948,  0.4105,  0.7182,\n",
      "         0.3595,  0.6303,  0.7492,  0.5521,  0.5746,  0.3578, -0.0190,  0.9274,\n",
      "         0.2742,  0.1476,  0.2406,  0.3628,  0.4523,  0.3634,  0.4850,  0.3599,\n",
      "         0.5240,  0.1677,  0.9379,  0.4969,  0.4776,  0.4500,  0.3288,  0.3787,\n",
      "         0.3845,  0.1362,  0.2795,  0.4424,  0.5441,  0.2259,  0.1552,  0.3783,\n",
      "         0.1311])\n",
      "tensor([0.4269, 0.2815, 0.0705, 0.0231, 0.0000, 0.2753, 6.7727, 0.4919, 0.1215,\n",
      "        0.2277, 0.0000, 0.1750, 0.1892, 0.2461, 0.0440, 0.4887, 0.1094, 0.7061,\n",
      "        0.3923, 0.5664, 3.0380, 0.2893, 1.1059, 0.7952, 0.8124, 0.2480, 0.3555,\n",
      "        0.4714, 0.3045, 0.0596, 0.2931, 0.3485, 0.2860, 0.3199, 0.4694, 0.2368,\n",
      "        0.3005, 0.0953, 0.3710, 0.1707, 0.0613, 0.0622, 0.0928, 0.0542, 0.2762,\n",
      "        0.1105, 0.1634, 0.1959, 0.1476, 0.0855, 0.1319, 1.8150, 0.3358, 0.5921,\n",
      "        1.6402, 0.4272, 0.2233, 0.1738, 0.1885, 0.8457, 0.1085, 0.6625, 0.0000,\n",
      "        2.4377, 0.8317, 0.1061, 0.2984, 0.1166, 4.4294, 0.7532, 0.0422, 0.0000,\n",
      "        0.2093, 0.1714, 0.5188, 0.4648, 0.4714, 0.1350, 0.1443, 0.1632, 0.0000,\n",
      "        0.1401, 0.2148, 0.5542, 0.0000, 0.2662, 0.0425, 0.1992, 0.2637])\n",
      "(0.11007443552805618, 0.30447481581857955)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for d in dl_dev_oneBatch:\n",
    "        pred_dev = model(input_ids=d[\"input_ids\"].to(device), attention_mask=d[\"attention_mask\"].to(device))[0] # just the logits?\n",
    "        y_dev = d[\"target\"].to(device)\n",
    "        \n",
    "        pred_dev = pred_dev.squeeze().cpu()\n",
    "        y_dev = y_dev.squeeze().cpu()\n",
    "        print(pred_dev)\n",
    "        print(y_dev)\n",
    "        print(st.pearsonr(pred_dev, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kommentare:\n",
    "* ist jetzt auf GPU\n",
    "* Overfitting! Dropout? zu wenig Daten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
